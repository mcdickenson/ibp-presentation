\documentclass[13pt]{beamer}
\usetheme{Madrid}
\setbeamercovered{invisible}
\setbeamertemplate{navigation symbols}{} 
\usepackage{coordsys} % for number lines
\usepackage{graphicx}
\usepackage{pgfkeys}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{subfloat}
\usepackage{tikz}
\usetikzlibrary{matrix}

% font customization
% \usepackage{mathspec}
% \usepackage{xunicode}
% \usepackage{xltxtra}
% \setmainfont{Gill Sans}
% \setmathsfont(Digits,Latin,Greek){Gill Sans}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title[Indian Buffet Process]{The Indian Buffet Process}
\author[Bingham and Dickenson]{Eli Bingham\inst{1} \and Matt Dickenson\inst{2}}
\institute[UNC and Duke]{\inst{1} University of North Carolina \and \inst{2} Duke University}
\date{February 10, 2014}


\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\titlepage
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Outline}
\begin{enumerate}
\item Introduction
\item Dirichlet and Chinese Restaurant Processes
\item Beta and Indian Buffet Processes
\item Gibbs sampling
\item Demonstration/Visualization
\item Applications: Choice Behavior and Collaborative Filtering
\item Extensions: Topic Models and Cascading IBP
\item Discussion
\end{enumerate}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Latent feature models}
\begin{itemize}
\item Feature model: $N$ items described by $K$ features
\item Dense feature model: every feature is present in every item, e.g. PCA
\item Sparse feature model: only some features present in each item, and we can assume feature values and presence are independent:
    \begin{eqnarray*}
        \mathbf{F} &=& \mathbf{A} \otimes \mathbf{Z} \\
        P(\mathbf{F}) &=& P(\mathbf{A})P(\mathbf{Z})
    \end{eqnarray*}
\end{itemize}

\begin{figure}
\begin{center}
\includegraphics[scale=0.4]{./img/featuremodel.png}
\caption{Griffiths and Ghahramani (2011) Figure 3}
\end{center}
\end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Example}

\begin{figure}
\begin{center}
\includegraphics[scale=0.3]{./img/graphical-model-gaussian.png}
\caption{Griffiths and Ghahramani (2011) Figure 7}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.3]{./img/ibp-example-photos.png}
\caption{Griffiths and Ghahramani (2011) Figure 9}
\end{center}
\end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Motivation}
\begin{itemize}
\item Problem with finite latent feature model: $K$ is fixed
\item Goal: construct nonparametric prior on $\mathbf{Z}$ so that $K$ grows with the complexity of the dataset
\item As with DPMMs, we can try to build one by taking $K \rightarrow \infty$ in a finite feature model
\end{itemize}

\begin{figure}
\begin{center}
\includegraphics[scale=0.4]{./img/beta-bernoulli.png}
\caption{Griffiths and Ghahramani (2011) Figure 4}
\end{center}
\end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Background: Dirichlet Process}

Finite version (Dirichlet distribution):
\begin{itemize}
\item Assignment of an object to a class is independent of all other assignments: $P(c|\theta) = \prod_{i=1}^N P(c_i|\theta) = \prod_{i=1}^N \theta_{c_i}$
\item $\theta|\alpha \sim \text{Dirichlet}(\frac{\alpha}{K},\ldots,\frac{\alpha}{K})$ (if symettric)
\item $c_i|\theta \sim \text{Discrete}(\theta)$, where Discrete : Bernoulli :: Multinomial : Binomial
\end{itemize}

Integrating out $\theta$: $P(c) = { \prod_{k=1}^K \Gamma(m_k + \frac{\alpha}{K}) \over \Gamma(\frac{\alpha}{K})^K} {\Gamma(\alpha) \over \Gamma(N+\alpha)}$

As $K \rightarrow \infty$, we get the CRP

\begin{figure}
\begin{center}
\includegraphics[scale=0.3]{./img/crp-graphical-model.png}
\caption{Griffiths and Ghahramani (2011) Figure 1}
\end{center}
\end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Background: Chinese Restaurant Process}

\begin{enumerate}
\item $N$ customers enter (in sequence) a restaurant with an infinite number of tables, each with infinite seating
\item First customer sits at first table with probability $\frac{\alpha}{\alpha}=1$
\item $i^{th}$ customer sits at the $k^{th}$ table with probability $\frac{m_k}{i+\alpha-1}$, where $m_k$ is the number of previous customers who sat at table $k$, or a new table with probability $\frac{\alpha}{i+\alpha-1}$
\end{enumerate}

\begin{figure}
\begin{center}
\includegraphics[scale=0.3]{./img/crp-example.png}
\caption{Griffiths and Ghahramani (2011) Figure 2}
\end{center}
\end{figure}

Limitation: each object (customer) can only belong to one class (table).

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Finite latent feature models}
The basic finite distribution on $z_{i,k}$s: 
\begin{eqnarray*}
    \pi_k | \alpha &\sim& \text{Beta}(\frac{\alpha}{K}, 1) \\
    z_{i,k} | \pi_k &\sim& \text{Bernoulli}(\pi_k)
\end{eqnarray*}
As with DPMMs, we can marginalize out latent feature presence probabilities $\pi_k$ to obtain a distribution on matrices $\mathbf{Z} \in \left\{ 0,1 \right\}^{N \times K}$:
\begin{eqnarray*}
    P(\mathbf{Z}) &=& \prod_{k=1}^K \int \left( \prod_{i=1}^N P(z_{ik} | \pi_k) \right) P(\pi_k) d \pi_k \\
    &=& \prod_{k=1}^K \frac{ \frac{\alpha}{K} \Gamma(m_k + \frac{\alpha}{K} ) \Gamma(N - m_k + 1) }{\Gamma(N + 1 + \frac{\alpha}{K}) }
\end{eqnarray*}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{The $K \rightarrow \infty$ limit}
\begin{itemize}
\item $lof(\mathbf{Z})$ is the matrix obtained by ordering the columns of $\mathbf{Z}$ as $N$-digit binary numbers
\item To define a probability over infinitely wide binary matrices using de Finetti's Theorem,
we need exchangeable symmetry, so we define \emph{lof} equivalence classes by modding out column order:
\end{itemize}

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{./img/ibp-sorted.png}
\caption{Griffiths and Ghahramani (2011) Figure 5}
\end{center}
\end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Indian Buffet Process}

Indian Buffet Process:
\begin{enumerate}
\item $N$ customers enter (in sequence) a buffet restaurant with an infinite number of dishes
\item First customer fills her plate with Poisson($\alpha$) number of dishes
\item $i^{th}$ customer samples dishes in proportion to their popularity, with probability $\frac{m_k}{i}$, where $m_k$ is the number of previous customers who sampled dish $k$
\item $i^{th}$ customer then samples $K^{(i)}_1 \sim \text{Poisson}(\frac{\alpha}{i})$ number of new dishes
\end{enumerate}
\bigskip
Resulting probability distribution on matrices: $$P(\mathbf{Z}) = \frac{\alpha^{K_+}}{\prod_{i=1}^N K^{(i)}_1 ! } \text{exp}(\alpha H_N) \prod_{k=1}^{K_+} \frac{(N - m_k)! (m_k - 1)!}{N!} $$
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Demo}

\begin{center}
\href{http://mcdickenson.shinyapps.io/ibp-demo}{mcdickenson.shinyapps.io/ibp-demo}
\includegraphics[scale=0.4]{./img/ibp-shiny-demo.png}
\end{center}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Alternative derivation: Beta Process}
% p 1194-5
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Alternative derivation: Stick-Breaking}

\begin{enumerate}
\item Recursively break (an initially unit-length) stick, breaking off a $\text{Beta}(\alpha, 1)$ portion at each step
\item Let each portion of the ``stick'', $\pi_k$ represent the probability of each feature (sorted from largest to smallest)
\end{enumerate}

This helps to show the relation between the Dirichlet process and the IBP. The stick-breaking construction is also useful for defining inference algorithms.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Properties of the Resulting Distribution}
% 4.6
\begin{itemize}
\item The ``effective'' dimension $K_+ \sim \text{Poisson}(\alpha H_N)$
\item The number of dishes on each customer's plate is distributed Poisson($\alpha$) (by exchangeability)
\item $\bf{Z}$ remains sparse as $K\rightarrow \infty$: effective dimensions of $\bf{Z}$ are $N \times K_+$, and the expected number of entries is $N\alpha$
\end{itemize}
% note that the third item here is a limitation of the IBP: average number of features $\alpha$ and total number of features $N\alpha$ are directly coupled

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Inference by Gibbs Sampling}
Given some data $\mathbf{X}$, we want to sample from a marginal posterior 
\[
    P(z_{i,k} = 1 | \mathbf{Z}_{-(ik)},\mathbf{X} ) \propto P(\mathbf{X}| \mathbf{Z}) P(z_{i,k} = 1 | \mathbf{Z}_{-(ik)} )
\]

Iterate continuously over the rows $\mathbf{z}_i$ ($i=1\ldots N$) of $\mathbf{Z}$:
\begin{enumerate}
    \item For each column $k$ of $\mathbf{Z}$, if $m_{-i,k} = 0$ (i.e. the rest of the column is empty) delete the column; 
        otherwise set $z_{i,k}=1$ with probability $$P(\mathbf{X}|\mathbf{Z}) P(z_{i,k}=1 | \mathbf{z}_{-i,k}) =  P(\mathbf{X}|\mathbf{Z})\frac{m_{-i,k}}{N}$$
    \item At the end of the row, add $K_1^{(i)} \sim P(\mathbf{X}|\mathbf{Z}) \text{Poisson}(\frac{\alpha}{N})$ new columns with ones in row $i$
\end{enumerate}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Variational Inference}
% sec 6.9
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Application 1: Choice Behavior}

% 6.1
\begin{center}
``A Choice Model with Infinitely Many Latent Features'' \\ (G\"{o}r\"{u}r, J\"{a}kel, and Rasmussen, ICML 2006)
\end{center}

\begin{itemize}
\item Customers compare items (e.g. cell phones) based on the (binary) features of each; more features are better
\item Number of features is potentially infinite and ordering is not important, so IBP is used
\item Celebrity example: ``With whom would you prefer to spend an hour of conversation?'' 
\end{itemize}

% \begin{eqnarray*}
% P(f_{ik}=1|f_{-i,k}) &=& {m_{-i,k} + \alpha/K \over N + \alpha/K} \\
% \lim_{K \rightarrow \infty} P(f_{ik}=1|f_{-i,k}) &=& {m_{-i,k} \over N}
% \end{eqnarray*}

\begin{figure}
\begin{center}
\includegraphics[scale=0.3]{./img/celebrity-example.png}
\caption{G\"{o}r\"{u}r, J\"{a}kel, and Rasmussen (2006) Figure 3}
\end{center}
\end{figure}
% ``We showed empirically that the infinite model (iEBA) can capture the latent structure in the choice data as well as the handcrafted model (tEBA). For data for which we have less prior informa- tion it might not be possible to handcraft a reasonable feature matrix.''

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Application 2: Collaborative Filtering}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Extension 1: Topic Modeling}
% Williamson, Wang, Heller, and Blei (2010)

\begin{center}
``The IBP Compound Dirichlet Process \\
and its Application to Focused Topic Modeling'' \\
Williamson, Wang, Heller, and Blei (2010)
\end{center}

Stick-breaking construction:

\begin{eqnarray*}
\mu_k \sim \text{Beta}(\alpha, 1) \\
\pi_k = \prod_{j=1}^k \mu_j \\
b_{m,k} \sim \text{Bernoulli}(\pi_k) 
\end{eqnarray*}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Extension 1: Topic Modeling}
% Williamson, Wang, Heller, and Blei (2010)

% \begin{center}
% ``The IBP Compound Dirichlet Process \\
% and its Application to Focused Topic Modeling'' \\
% Williamson, Wang, Heller, and Blei (2010)
% \end{center}

Focused topic model:
\begin{enumerate}
\item for $k=1,2,\ldots$
  \begin{itemize}
  \item Sample stick length $\pi_k$
  \item Sample relative mass $\phi_k \sim \text{Gamma}(\gamma, 1)$
  \item Draw topic distribution over words: $\beta_k \sim \text{Dirichlet}(\eta)$
  \end{itemize}
\item for $m=1,\ldots,M$
  \begin{itemize}
  \item Sample binary vector $b_m$
  \item Draw total number of words $n^{(m)} \sim NB(\sum_k b_{m,k} \phi_k, 1/2)$
  \item Sample distribution over topics $\theta_m \sim \text{Dirichlet}(b_m \cdot \phi)$
  \item For each word $w_{m,i}, i=1,\ldots,n^{(m)}$
    \begin{enumerate}
    \item Draw topic index $z_{m,i} \sim \text{Discrete}(\theta_m)$ 
    \item Draw word $w_{m,i} \sim \text{Discrete}(\beta_{z_{m_i}})$
    \end{enumerate}
  \end{itemize}
\end{enumerate}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Extension 1: Topic Modeling}
% Williamson, Wang, Heller, and Blei (2010)

% \begin{center}
% ``The IBP Compound Dirichlet Process \\
% and its Application to Focused Topic Modeling'' \\
% Williamson, Wang, Heller, and Blei (2010)
% \end{center}

An advantage of the focused topic model is that it separates the global topic proportions from the distribution over topics within a topic. A rare topic within the corpus can be dominant within a document (e.g. baseball), and a frequent topic can be a small proportion of many documents.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Extension 2: Collaborative Filtering}
% Adams, Wallach, and Ghahramani
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Discussion}


Limitations of IBP:
\begin{enumerate}
\item Coupling of average number of features $\alpha$ and total number of features $N\alpha$ (can be overcome with a two-parameter generalization)
\item Computationally complex, can be time-consuming
\end{enumerate}

\begin{figure}
\begin{center}
\includegraphics[scale=0.3]{./img/two-parameter-ibp.png}
\caption{Griffiths and Ghahramani (2011) Figure 10}
\end{center}
\end{figure}

\end{frame}
 
% End of slides
\end{document} 
